{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1900145,"sourceType":"datasetVersion","datasetId":693124},{"sourceId":10505870,"sourceType":"datasetVersion","datasetId":6503686}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision timm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T11:02:00.273786Z","iopub.execute_input":"2025-01-18T11:02:00.274182Z","iopub.status.idle":"2025-01-18T11:02:05.755194Z","shell.execute_reply.started":"2025-01-18T11:02:00.274151Z","shell.execute_reply":"2025-01-18T11:02:05.753849Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.27.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.67.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.12.14)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# EVALUATIONS","metadata":{}},{"cell_type":"code","source":"from timm.layers.helpers import to_2tuple\nimport timm\nimport torch.nn as nn\n\nclass ConvStem(nn.Module):\n  \"\"\"Custom Patch Embed Layer.\n\n  Adapted from https://github.com/Xiyue-Wang/TransPath/blob/main/ctran.py#L6-L44\n  \"\"\"\n\n  def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=768, norm_layer=None, **kwargs):\n    super().__init__()\n\n    # Check input constraints\n    assert patch_size == 4, \"Patch size must be 4\"\n    assert embed_dim % 8 == 0, \"Embedding dimension must be a multiple of 8\"\n\n    img_size = to_2tuple(img_size)\n    patch_size = to_2tuple(patch_size)\n\n    self.img_size = img_size\n    self.patch_size = patch_size\n    self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n    self.num_patches = self.grid_size[0] * self.grid_size[1]\n\n    # Create stem network\n    stem = []\n    input_dim, output_dim = 3, embed_dim // 8\n    for l in range(2):\n      stem.append(nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=2, padding=1, bias=False))\n      stem.append(nn.BatchNorm2d(output_dim))\n      stem.append(nn.ReLU(inplace=True))\n      input_dim = output_dim\n      output_dim *= 2\n    stem.append(nn.Conv2d(input_dim, embed_dim, kernel_size=1))\n    self.proj = nn.Sequential(*stem)\n\n    # Apply normalization layer (if provided)\n    self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n  def forward(self, x):\n    B, C, H, W = x.shape\n\n    # Check input image size\n    assert H == self.img_size[0] and W == self.img_size[1], \\\n        f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n\n    x = self.proj(x)\n    x = x.permute(0, 2, 3, 1)  # BCHW -> BHWC\n    x = self.norm(x)\n    return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T11:28:24.670264Z","iopub.execute_input":"2025-01-19T11:28:24.670697Z","iopub.status.idle":"2025-01-19T11:28:41.221785Z","shell.execute_reply.started":"2025-01-19T11:28:24.670664Z","shell.execute_reply":"2025-01-19T11:28:41.219726Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimport torch\n\n# Charger l'image\nimg_path = \"/kaggle/input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_LymphNodes_01.tif\"\nimg = Image.open(img_path).convert(\"RGB\")  # Convertir en RGB pour être compatible avec le modèle\n\n# Charger le modèle préentraîné depuis Hugging Face Hub via timm\nmodel = timm.create_model(\n    model_name=\"hf-hub:1aurent/swin_tiny_patch4_window7_224.CTransPath\",\n    pretrained=True,\n).eval()\n\n# Obtenir les configurations spécifiques au modèle pour les transformations\ndata_config = timm.data.resolve_data_config({}, model=model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\n# Appliquer les transformations sur l'image\ndata = transforms(img).unsqueeze(0)  # Ajouter la dimension batch\n\n# Vérifier si un GPU est disponible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\ndata = data.to(device)\n\n# Effectuer l'inférence\nwith torch.no_grad():\n    output = model(data)\n\n# Afficher la sortie\nprint(\"Output shape:\", output.shape)\nprint(\"Output:\", output)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport timm\nfrom timm.models.swin_transformer import SwinTransformer\nfrom torch import nn\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Define the ConvStem to match the checkpoint's architecture\nclass ConvStem(nn.Module):\n    def __init__(self, in_channels=3, embed_dim=96):\n        super().__init__()\n        self.proj = nn.Sequential(\n            nn.Conv2d(in_channels, embed_dim // 2, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(embed_dim // 2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(embed_dim),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.proj(x)\n\n# Create a custom Swin Transformer that uses the ConvStem\nclass CustomSwinTransformer(SwinTransformer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.patch_embed.proj = ConvStem(in_channels=3, embed_dim=self.embed_dim)\n\n# Load the model with the adjusted architecture\nmodel = CustomSwinTransformer(\n    patch_size=4,\n    window_size=7,\n    embed_dim=96,\n    depths=(2, 2, 6, 2),\n    num_heads=(3, 6, 12, 24),\n    num_classes=1000\n)\n\n# Load the pretrained weights\ncheckpoint_path = \"/kaggle/input/ctranspath-models/checkpoint.pth\"  # Update with the actual path\ncheckpoint = torch.load(checkpoint_path, map_location=\"cpu\")\nmodel.load_state_dict(checkpoint, strict=False)  # Load with non-strict mode to handle differences\n\n# Move the model to GPU and set to evaluation mode\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()\n\n# Define transforms\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\n# Load and preprocess the image\nimg_path = \"/kaggle/input/segmentation-of-nuclei-in-cryosectioned-he-images/tissue images/Human_LymphNodes_01.tif\"\nimg = Image.open(img_path).convert(\"RGB\")\ndata = transforms(img).unsqueeze(0).to(device)  # Add batch dimension and move to device\n\n# Perform inference\nwith torch.no_grad():\n    output = model(data)  # Output is (batch_size, num_features)\n    print(\"Model output:\", output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:17:23.219909Z","iopub.execute_input":"2025-01-19T12:17:23.221658Z","iopub.status.idle":"2025-01-19T12:17:26.209844Z","shell.execute_reply.started":"2025-01-19T12:17:23.221608Z","shell.execute_reply":"2025-01-19T12:17:26.208401Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-15-77b861ac40ff>:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"Model output: tensor([[ 8.1148e-01,  1.8935e-01, -9.0224e-02,  2.5861e-01,  3.0596e-01,\n          8.2276e-03,  9.0792e-02,  6.6537e-01, -4.3929e-01,  4.7570e-01,\n         -3.9093e-01, -4.5301e-01, -4.3061e-01, -2.6897e-01,  1.2209e-01,\n         -9.3461e-02,  2.3282e-02, -5.1481e-01,  7.7189e-01,  2.6584e-02,\n         -2.7158e-01,  4.8113e-01,  1.1812e+00,  1.3154e-01,  5.9637e-01,\n          2.2954e-01,  1.3701e-01, -1.1506e+00, -2.3190e-01, -2.2726e-01,\n          9.2835e-02, -1.0608e-01, -4.6960e-01,  1.2167e+00,  5.3204e-01,\n         -2.2739e-01,  6.7872e-01,  4.4365e-01, -4.0916e-01, -6.2200e-02,\n          8.5042e-01, -4.2307e-01,  1.0337e+00,  7.4152e-02, -5.5259e-01,\n         -1.5084e-01, -4.5673e-01,  1.6890e-01, -6.2315e-01,  3.0302e-01,\n         -4.0741e-01,  9.9701e-01, -3.5055e-01,  6.4751e-01,  2.1747e-02,\n         -1.0295e-01, -4.4445e-01,  3.9062e-01, -5.9998e-01,  5.9981e-03,\n         -8.1216e-01,  1.1108e-01, -4.3611e-01,  3.6392e-01, -6.4813e-01,\n          1.9192e-01,  5.4796e-01, -3.3715e-01,  2.9531e-02, -1.2440e+00,\n          3.2949e-01, -2.1686e-01,  3.8405e-01,  1.3993e-01, -2.0960e-01,\n          6.1831e-01, -9.1308e-02,  2.2950e-01, -3.3202e-01,  7.8797e-02,\n          1.9897e-01, -4.7787e-02,  4.3616e-01,  7.4977e-01,  2.4743e-01,\n          2.5675e-02, -2.4928e-01, -7.1954e-01,  1.4696e-01, -3.9595e-01,\n          9.2385e-01, -3.0905e-01, -1.1815e+00,  1.5043e-01, -9.2991e-01,\n          8.3106e-01, -1.0680e+00, -8.6593e-01,  6.1338e-01,  3.4184e-01,\n         -3.1588e-02, -1.2866e-01, -5.5356e-01,  2.5494e-01,  7.7812e-01,\n          4.1651e-01, -4.0964e-01,  1.6282e-01, -5.1287e-01,  7.3320e-01,\n          2.4611e-01, -1.2873e-01, -2.7684e-01, -6.6731e-01, -1.8342e-01,\n          2.9924e-01,  2.5086e-02,  7.2659e-01, -1.1112e-01,  8.1633e-01,\n         -2.1043e-01,  2.0247e-01,  1.3204e-03,  7.4014e-01,  2.7378e-01,\n         -1.8867e-01,  1.9023e-02, -3.9078e-01, -5.5899e-02, -5.1985e-02,\n         -5.5518e-01,  3.7730e-01,  5.4385e-01,  4.9725e-02, -2.8708e-01,\n          8.0397e-02,  4.1790e-01, -6.9638e-01, -7.9845e-01,  7.6186e-01,\n         -4.7748e-01,  6.1747e-01, -1.1336e-01,  2.6990e-01,  5.1903e-01,\n         -8.1183e-01,  1.1881e-01, -7.4697e-01, -1.0791e+00, -6.6540e-01,\n          4.4645e-01, -1.3061e-02, -2.2214e-01, -1.1352e-01, -3.1000e-01,\n          1.0409e-01,  3.2332e-01,  6.2752e-01, -1.1110e+00, -1.6556e-01,\n          5.2461e-01,  1.8624e-01,  4.6270e-02, -5.5114e-01,  4.9566e-01,\n          6.7195e-01, -3.6221e-01, -1.0892e+00, -2.5612e-01, -2.3446e-01,\n         -5.7481e-01, -1.0004e+00, -3.7496e-02, -7.7018e-01,  2.1573e-01,\n         -5.8433e-01,  2.2028e-01,  1.9279e-01,  3.2323e-02, -5.6954e-01,\n          4.5929e-01,  5.1921e-01,  1.9389e-01,  3.5235e-01,  6.4295e-01,\n          1.0854e+00, -4.2635e-01, -7.8699e-02, -3.3927e-01, -1.0844e-01,\n          1.1263e-01, -7.0172e-01,  3.9574e-02, -4.3262e-03, -3.1125e-01,\n          3.5822e-01, -5.0102e-01, -6.6142e-01, -3.7114e-01,  5.3718e-01,\n          5.0473e-01,  7.1929e-03, -3.6784e-01,  5.1910e-01,  6.6699e-02,\n          1.8163e-01, -8.0995e-01,  6.6386e-01, -1.5022e-01,  1.8354e-01,\n          4.9335e-01,  6.6560e-01,  9.9418e-02,  1.6610e-01, -9.3605e-02,\n          2.7350e-02, -2.6257e-01,  4.4205e-01, -2.4903e-01, -5.5126e-01,\n         -4.3307e-01, -3.1779e-01, -1.7673e-01, -1.8020e-01, -1.5425e-01,\n         -6.3154e-01, -4.0346e-01,  2.9481e-01,  7.5835e-01,  1.5905e-01,\n         -2.6772e-01,  1.5006e-01,  3.3187e-02,  1.5327e-01,  2.4901e-01,\n         -4.6446e-01,  1.3479e-01,  8.7673e-01, -1.9458e-01,  4.1130e-01,\n          6.7456e-01,  9.1741e-01,  4.5547e-02, -1.9056e-01,  3.9652e-01,\n         -4.6682e-01, -6.9850e-02,  1.8639e-01,  1.6976e-01,  1.2850e-01,\n         -2.3244e-01,  4.7790e-01,  2.9943e-01,  6.6537e-03, -1.2439e-01,\n         -2.2031e-01,  8.4943e-02, -8.5993e-01, -1.0541e+00, -1.0229e-01,\n         -4.8635e-01,  9.0726e-01,  8.1489e-01, -2.0106e-01, -1.5397e-01,\n         -8.4406e-03,  3.6803e-01,  3.1248e-01,  1.6553e-01,  3.6187e-01,\n          4.8199e-02, -6.9698e-01,  6.0387e-01,  4.0858e-01, -5.1678e-01,\n          6.1672e-01,  2.9117e-01,  5.0302e-01, -2.8604e-01, -3.6770e-01,\n          4.9789e-02, -1.2082e-01,  6.3002e-02,  3.5095e-02,  4.7367e-01,\n          4.3108e-02,  6.6585e-01, -8.9582e-02,  2.8435e-01,  1.7865e-01,\n          1.8081e-01, -3.3866e-02, -1.6100e-01,  2.9346e-01, -7.1459e-01,\n         -6.4509e-01,  4.8088e-01,  4.8310e-01,  5.5217e-01,  6.5102e-02,\n         -4.5237e-02, -2.0657e-01,  2.6551e-01, -6.5618e-01,  1.2858e-01,\n          6.9479e-01, -4.3202e-01,  4.1462e-01, -1.6040e-01, -5.5990e-01,\n         -9.5189e-02,  7.3068e-01, -1.7317e+00, -6.2302e-01,  6.7226e-02,\n          1.6461e-01, -1.5169e-01,  2.0450e-01,  3.7468e-01, -3.5928e-03,\n         -5.8489e-01,  7.6049e-01, -4.2193e-02, -4.7491e-01,  4.0302e-01,\n          3.6140e-01, -4.2374e-01,  7.2585e-01, -3.0421e-01, -3.5972e-01,\n         -4.3542e-01, -1.2344e-01, -3.7669e-02,  1.9383e-01,  5.1442e-01,\n          9.9530e-02,  1.2488e-01,  2.5288e-02, -8.7046e-02,  5.5737e-01,\n         -5.0970e-01, -6.7037e-02,  1.5173e-01, -4.3595e-01,  4.6299e-01,\n          9.7518e-01, -3.3505e-02,  8.8334e-01,  4.2358e-01,  4.9732e-01,\n          9.3730e-01, -4.7020e-01, -9.1107e-01, -1.3753e-01,  6.8465e-01,\n          4.9086e-01,  1.1521e-01, -1.0960e-01,  7.7163e-02, -9.7757e-01,\n          8.1897e-02, -2.8974e-01,  1.6690e-01,  1.2695e-01,  3.0673e-02,\n          5.7496e-01,  4.7091e-01,  8.6504e-03,  7.6320e-02,  1.9335e-02,\n         -1.3941e-01, -1.1772e+00,  5.6389e-01, -5.6485e-02,  3.0225e-01,\n          7.1384e-01, -3.4174e-01, -4.5725e-02,  1.2452e-01,  9.7731e-01,\n         -3.3811e-01, -1.4479e-03,  4.8049e-05,  2.2777e-01, -5.9714e-02,\n          7.2453e-02,  1.2543e-02, -4.4002e-01, -2.4719e-01, -3.6241e-01,\n         -8.9735e-01,  3.0603e-02,  3.2393e-01,  8.8973e-02,  1.0999e+00,\n         -3.7795e-01, -9.5896e-01,  5.2360e-01, -4.3571e-01, -2.0777e-01,\n          4.0501e-01, -4.8187e-01, -5.5525e-01,  1.6381e-02, -1.3435e-01,\n         -7.8193e-01,  3.8689e-01, -6.0181e-01,  2.0189e-01,  1.3799e+00,\n         -1.2226e+00, -6.5056e-01, -7.0920e-01,  5.0825e-01,  1.5452e-01,\n         -4.2136e-01,  4.4226e-02,  2.3623e-01,  5.2923e-01,  3.5340e-01,\n          5.4791e-01, -4.2737e-01,  1.6885e-01, -7.6582e-02,  4.2616e-02,\n         -1.1415e-01, -6.4237e-01,  3.4124e-01, -1.3090e-01,  8.1299e-02,\n          4.3441e-01, -5.2782e-01, -1.7179e-01,  2.5954e-01,  1.5584e-01,\n         -4.5264e-01,  1.7833e-01, -2.9865e-01, -3.3269e-01, -2.7676e-01,\n         -6.1955e-01,  2.3300e-01, -5.6990e-01,  3.8602e-01, -6.2056e-01,\n          1.9123e-01,  7.2637e-01, -1.8823e-01, -3.0445e-01,  5.2784e-01,\n         -3.0935e-02,  6.3660e-01,  2.1726e-01,  6.2920e-01, -5.6572e-02,\n         -6.3443e-01,  3.4877e-02, -3.1249e-02, -3.9245e-01,  1.0915e-01,\n          3.3008e-02,  1.5574e-01,  1.2775e-01,  2.2727e-01, -9.5049e-01,\n         -4.6323e-02, -3.0461e-01,  3.1397e-01,  3.9911e-02, -1.9077e-01,\n         -5.3266e-01, -7.8026e-01, -9.3689e-01, -4.2078e-01, -8.6268e-01,\n          4.7138e-01,  8.6170e-03, -2.8660e-01, -6.1545e-01, -5.2706e-01,\n         -2.4284e-01,  7.7945e-02, -8.8080e-01, -1.6471e-01,  5.5754e-01,\n         -6.0876e-03, -1.4761e-03, -5.9166e-01, -4.1938e-01,  5.2822e-01,\n         -4.6550e-01, -4.5490e-01, -5.6150e-01,  5.7460e-01, -4.8683e-01,\n          1.4787e-01,  6.7021e-01, -8.8550e-01,  5.7718e-02, -2.1030e-01,\n         -1.5131e-02,  4.7452e-01,  3.8262e-01,  8.1272e-02,  8.1054e-01,\n          7.6521e-01,  5.3898e-01, -4.8260e-02, -9.8355e-01,  2.1732e-01,\n         -2.0943e-01, -4.7865e-03, -2.6150e-01,  5.0058e-01,  7.4605e-01,\n          7.5095e-01,  4.7328e-01,  3.5601e-01,  1.0045e+00, -1.2780e-01,\n         -1.5406e-01,  1.6076e-02, -8.7104e-01, -3.2909e-01,  1.1850e-01,\n          4.3014e-01, -7.6404e-02, -2.5589e-01, -1.1687e-01,  5.9515e-03,\n         -5.0376e-01,  1.6494e-02,  3.4625e-01, -2.0113e-01, -1.0125e+00,\n         -5.3084e-01, -2.1459e-01,  3.1633e-01, -1.1554e-01, -5.6174e-02,\n          6.2978e-02,  2.9957e-01, -2.2597e-01, -2.5079e-01,  6.5280e-01,\n          1.8056e-01,  5.1612e-02,  3.6523e-01, -4.2890e-01, -4.4641e-01,\n         -8.8903e-01, -1.8105e-01,  1.9944e-02,  2.1260e-02, -4.1340e-01,\n          3.2160e-01, -4.4606e-01,  6.8419e-01, -1.0250e+00,  6.1394e-01,\n         -3.1442e-01,  4.9777e-01,  3.5321e-01, -5.6325e-02,  7.1794e-02,\n          4.8789e-04, -2.2355e-01, -5.7701e-01,  7.3187e-01, -4.4113e-01,\n          5.0437e-01,  1.0202e-01, -4.2628e-01,  2.4493e-01, -5.2338e-02,\n         -6.5656e-01,  1.8761e-01, -5.4806e-02,  3.7570e-01,  9.5027e-02,\n         -5.5842e-01, -5.8142e-01,  9.6793e-01, -8.2486e-01, -7.5843e-01,\n         -2.5103e-02,  1.1644e-01,  1.4997e-01, -3.9446e-01,  2.0696e-01,\n          5.5769e-01, -2.3546e-01,  1.8908e-01, -9.4614e-01, -2.4627e-01,\n         -5.8113e-01,  1.1577e-01,  6.9653e-01,  5.9852e-01,  1.2335e+00,\n         -4.0913e-01, -7.0040e-01,  1.1570e-01, -1.7347e-01,  3.7109e-01,\n         -1.8123e-01,  1.0387e+00,  1.2655e-01,  8.5113e-01, -3.3232e-01,\n          1.1036e+00, -4.2038e-02, -5.0371e-01,  2.7620e-01,  5.5125e-02,\n         -1.3275e-01,  7.1506e-02,  3.9281e-01,  1.2589e-01, -4.6300e-01,\n         -8.6402e-02,  3.9360e-01,  4.7484e-01,  2.3323e-03,  4.6000e-01,\n          3.5817e-01, -6.3558e-01,  5.4268e-02,  3.2684e-01,  1.0757e-01,\n         -5.8613e-01,  2.3919e-01, -2.1371e-01, -6.9870e-01, -3.2329e-01,\n          3.5795e-01,  8.0618e-01, -5.6429e-01,  4.4410e-02,  1.5693e-01,\n         -6.7039e-01, -1.3212e-01, -1.1503e+00, -1.3107e-01, -3.4316e-01,\n          4.1514e-01, -5.9985e-02,  3.4317e-02,  6.5199e-02, -2.8466e-01,\n          7.3879e-01, -7.7166e-01,  8.5673e-02,  3.0524e-01, -3.8478e-01,\n          3.1969e-01,  3.6376e-02, -4.2579e-01, -3.9551e-01,  1.3477e-01,\n         -1.1934e+00,  6.0752e-01, -9.0856e-01, -2.8676e-01, -6.0736e-01,\n          4.3921e-01, -4.2349e-01,  1.2907e-01,  4.3676e-01,  5.8410e-01,\n          8.3063e-01,  1.4782e-01, -7.6848e-01, -2.2080e-01,  8.5761e-01,\n          6.5041e-03, -1.5786e-01, -4.6075e-02, -4.7293e-01,  7.2231e-01,\n         -1.2087e-01,  5.5124e-02,  4.8194e-01,  6.7253e-01, -8.6928e-02,\n         -5.6572e-01, -5.7688e-01, -5.2099e-01, -2.7195e-01, -6.4971e-01,\n         -2.9158e-01, -3.3454e-01,  4.9334e-01,  7.3204e-01, -9.6073e-01,\n         -7.8626e-01, -3.1948e-02,  5.5288e-03,  3.3460e-02, -6.5271e-01,\n         -1.6071e-01,  5.9376e-02, -3.6599e-02, -1.0408e-01,  3.8504e-01,\n          1.6519e-01, -2.5842e-01,  3.3574e-01,  2.9398e-01, -1.6703e-01,\n          1.5124e-01,  1.6033e-01, -4.0875e-02,  2.5535e-02, -9.5396e-01,\n          3.0531e-01,  6.6935e-01, -7.2629e-01,  7.8381e-02, -6.2857e-01,\n          8.8706e-01, -2.0288e-01, -8.4234e-01,  1.6761e-01,  6.7700e-03,\n         -1.5639e-01,  1.7859e-02,  4.7543e-01,  3.2922e-02, -7.1453e-01,\n         -1.8438e-01, -1.4765e-01,  1.0688e-01, -3.6628e-01,  1.2813e-01,\n         -2.7100e-01,  6.1017e-01, -1.2137e-01, -2.0922e-02,  1.3213e-01,\n          3.2711e-01, -2.9955e-01, -5.6100e-01, -7.5225e-01,  2.2896e-01,\n         -1.8149e+00, -4.0523e-01,  8.2942e-02, -7.4709e-03, -2.8827e-01,\n          6.1248e-01, -8.3205e-01,  8.8965e-01,  1.1756e-01,  8.1028e-02,\n          5.8759e-01, -2.1534e-01, -2.3467e-01, -5.0401e-03,  3.7073e-01,\n         -8.7973e-01,  3.4258e-01, -7.0594e-02,  1.2226e-01, -2.9887e-01,\n         -1.6877e-02,  3.8908e-01,  1.8247e-01, -1.1345e+00,  4.5065e-02,\n         -3.4109e-01, -1.5090e-01, -9.6486e-01, -2.3695e-01, -1.2274e-01,\n          5.8049e-01,  6.5393e-01, -5.3163e-01, -4.8747e-01, -1.0339e-01,\n         -6.6049e-02, -9.5970e-02, -6.3140e-01,  5.0717e-01,  3.8568e-01,\n          5.8622e-01,  1.3379e-01,  1.8762e-01,  3.9705e-01,  1.3219e-01,\n         -1.2212e-01, -1.4000e-01, -5.7692e-01, -9.4135e-02,  4.9741e-01,\n         -1.4214e-01,  3.3554e-01,  1.5587e-01,  5.5704e-02, -2.9813e-01,\n         -5.2574e-01,  1.0733e-01, -1.8956e-01, -1.8903e-01,  3.9130e-01,\n         -4.0664e-01,  1.3501e-01, -6.6755e-01, -5.4753e-01, -6.2870e-01,\n         -1.0474e-01, -4.1088e-01,  2.2767e-01, -2.6814e-01,  3.4091e-03,\n         -7.2953e-02, -2.3018e-01,  8.0607e-01, -8.4095e-01,  7.2521e-01,\n         -4.4909e-01,  2.4954e-01, -1.3672e-01, -6.4507e-01,  5.6819e-01,\n         -7.8900e-01, -3.3005e-01, -1.8170e-01,  2.4502e-01, -2.1313e-01,\n         -1.1905e-01, -1.9572e-01, -1.2624e+00, -2.0948e-01, -1.6009e-01,\n         -5.0915e-01, -6.7148e-01,  2.2622e-02, -6.5494e-01,  1.3553e-01,\n         -1.0640e+00, -2.0601e-01, -8.2031e-01, -2.9808e-01, -3.0066e-01,\n          2.2377e-01,  2.9727e-01, -1.8146e-01,  2.5774e-01, -4.7361e-01,\n          6.5951e-02, -6.1942e-01,  1.5270e-01,  2.3975e-01,  4.1954e-01,\n          4.1273e-01,  4.7481e-01, -4.5912e-01,  1.6622e-01,  4.4879e-01,\n         -1.1466e-01,  3.0721e-01,  9.2601e-01, -9.0589e-02, -1.4739e-01,\n          3.1555e-01, -3.4834e-01,  1.8663e-01, -4.8798e-01, -7.2823e-03,\n         -8.9783e-01,  9.0900e-02, -6.4124e-01,  3.7454e-01,  2.6344e-01,\n         -5.5623e-01,  2.8333e-01,  6.2772e-01,  3.5446e-01, -1.1669e-01,\n         -3.5334e-01, -4.7273e-01,  3.5061e-02, -1.5619e-01,  2.6516e-01,\n         -2.3537e-01, -6.5691e-02, -2.4693e-01, -1.2980e+00, -3.4824e-01,\n          1.2845e-01,  1.2166e-01,  8.2146e-02,  5.4665e-02,  3.0182e-03,\n          1.5273e-01,  3.7983e-02, -9.5581e-02,  1.6355e-01, -1.0623e-01,\n          9.9006e-01, -3.6657e-01, -2.6395e-01, -2.2020e-01,  2.8092e-01,\n         -7.5265e-01,  1.1185e-01,  2.5863e-01,  1.7733e-01,  1.6395e-01,\n         -3.2696e-01,  1.8837e-01, -1.9511e-01,  2.6938e-01,  6.6319e-01,\n         -8.1021e-01, -8.0748e-02,  1.9953e-01, -1.2953e-01,  4.5077e-02,\n          3.4887e-01,  7.0826e-02, -5.3613e-02,  3.7670e-01, -5.6253e-01,\n          8.7182e-01, -7.7110e-02, -6.1058e-01,  2.7498e-01,  5.1345e-01,\n         -6.9187e-01, -5.5968e-01,  2.3777e-01,  3.1374e-01, -6.4404e-01,\n          6.9597e-01,  1.9293e-01,  9.5152e-02,  1.6465e-02, -5.3605e-02,\n         -4.3398e-01, -8.8679e-02,  5.6171e-01,  4.2078e-03, -9.3935e-02,\n          6.8208e-03,  7.1953e-01, -6.0407e-01,  1.1775e+00,  1.9937e-01,\n         -2.7713e-01, -2.8434e-02,  6.3920e-02, -2.2861e-01, -1.5812e-01,\n          2.4816e-01, -2.3060e-01,  6.2480e-01, -3.1344e-01, -3.0486e-02,\n          7.3333e-01, -2.4068e-01,  1.3016e-01, -1.6559e-01, -5.3280e-01,\n          2.9347e-01,  2.1862e-01, -2.2502e-01,  1.4558e-01, -3.5883e-01,\n         -2.9046e-01,  9.5519e-01, -4.6294e-01,  6.3340e-01,  9.4815e-01,\n         -5.4416e-01, -7.8924e-01, -3.0296e-02,  3.1410e-02,  1.4786e-01,\n          5.7642e-01, -5.4767e-01,  8.5214e-01, -5.4468e-01, -3.5611e-02,\n          6.4137e-01,  2.9166e-01, -1.5707e-01, -4.3060e-01, -8.0599e-01,\n          6.0023e-01,  3.2692e-01, -6.5245e-03,  6.1997e-02,  5.5207e-01,\n         -9.7141e-01,  1.0099e-01, -3.2846e-01, -6.0339e-01, -1.0961e+00]])\n","output_type":"stream"}],"execution_count":15}]}